{"name":"Gummer","tagline":"Gummer, a framework for hunting APTs","body":"#Gummer\r\n## What is Gummer?\r\n\r\nGummer is a framework for hunting malware. It is based on anomalies and it is \r\nwritten in python.\r\n\r\nThe name of this framework comes from a very famous *gravoid* hunter called Burt\r\nGummer, who appears in all the *Tremors* movies.\r\n\r\n![BurtGummer.jpg](https://raw.githubusercontent.com/xgusix/gummer/master/doc/BurtGummer.jpg)\r\n\r\nAlso, this: [gummer](http://www.urbandictionary.com/define.php?term=gummer) @ \r\nurbandictionary.com\r\n\r\nGummer is basically a database query engine. It provides an easy way to have \r\nyour list of anomalies connected to your different databases, returning the\r\nresults in different formats.\r\n\r\nGummer was mainly created with the purpose of reducing the detection time of the\r\nincreasing number of the so-called APTs. It also happens to be very helpful for\r\nIncident Response tasks, when trying to contain an incident and not knowing\r\nwhere to start the analysis.\r\n\r\nIt’s important to highlight that using Gummer as an IDS is not recommended if\r\nyou are not specifically looking for this kind of threat. This is because the s\r\nearch of anomalies, or “weak signals”, can lead to a high number of false \r\npositives, which will result in people spending time reviewing the alerts.\r\n\r\nGummer is executed from the command line, it is multiplatform and it's in a\r\nbeta stage.\r\n\r\n## Using Gummer\r\n\r\nWhen we want to spot a specific anomaly, we run Gummer from the CLI, as follows:\r\n```\r\npython gummer.py –a <analyzer_id> -db <db_connector_id> -o <output_id>\r\n```\r\nIn the above command, we are passing the arguments of the different modules that\r\nGummer will use during its execution.\r\n\r\n## Gummer options:\r\n```\r\n$ python gummer.py -h\r\nusage: gummer.py [-h] [-l] [-ldb] [-lo] [-lc] [-a ANALYZER] [-db DBCONNECTOR]\r\n                 [-o OUTPUT] [-c COLLECTOR]\r\n\r\noptional arguments:\r\n  -h, --help            show this help message and exit\r\n  -l, --list_anal       List analyzers.\r\n  -ldb, --list_databases\r\n                        List db connectors.\r\n  -lo, --list_outputs   List possible output formats.\r\n  -lc, --list_collectors\r\n                        List collectors.\r\n  -a ANALYZER, --analyzer ANALYZER\r\n                        ID of the analyzer to use.\r\n  -db DBCONNECTOR, --dbconnector DBCONNECTOR\r\n                        ID of the database connector to use.\r\n  -o OUTPUT, --output OUTPUT\r\n                        ID of the output to use.\r\n  -c COLLECTOR, --collector COLLECTOR\r\n                        ID of the collector to use.\r\n```\r\n# The framework\r\n\r\nThis framework is based on a working instance of a similar product that I worked\r\nwith in the past. The product proved to work well, so I decided to write my own\r\nversion of it, making it easier to adapt to different environments.\r\n\r\nIn order to make it flexible, the framework is based on different modules: there\r\nis the main executable, gummer.py, which is supported by some other auxiliary\r\nfunctions, and there are the modules of the framework.\r\n\r\n![Gummer FW.png](https://raw.githubusercontent.com/xgusix/gummer/master/doc/modules.png)\r\n\r\n#Modules\r\n\r\nThe modules are small snippets of code that are imported on demand by\r\n*gummer.py*. They perform different tasks and are connected by *gummer.py* to\r\nperform the search of the anomalies on any of our databases and do whatever we\r\nwant with the output.\r\n\r\nThere are four kinds of modules: Collectors, Outputs, Analyzers and Database\r\nconnectors\r\n\r\n## Collectors\r\n\r\nThe first thing we need to do when working with Gummer is to feed data to it.\r\nTo do that, we use the collectors.\r\n\r\nA collector is a parser that processes the logs and inserts them into a database.\r\nTo start a collection process you must execute the following command:\r\n\r\n```\r\npython gummer.py –c <collector_id> -db <db_connector_id> -o <output_id>\r\n```\r\nThe general structure of a collector is the following:\r\n\r\n```python\r\nimport db_loader\r\n\r\naid = \"unique_id\"\r\nname = \"name of the collector\"\r\ndesc = \"Description of the collector\"\r\n\r\ndef launch(connector):\r\n    \"\"\"\r\n    Parselog and DB insertion\r\n    query = \"insert...\"\r\n    data = db_loader.db_query(connector, query)\r\n    \"\"\"\r\n    return suc, fail #Returns de number of successful and failed insertions.\r\n```\r\n\r\nWe need to import *db_loader* to be able to connect to the database from which\r\nwe are going to get the data. That is also the reason why the function *launch*\r\nreceives \"connector\". \"connector\" is the module that connects to the database\r\nfrom which the analyzer is going to retrieve the information.\r\n\r\nUsually (and up until now I haven’t found a different way to do this), when I\r\nwant to parse a log file I process it and insert the entries one by one in the\r\ndatabase. To do this, I use a loop where I keep count of the successful and\r\nfailed insertions. This count is what needs to be returned at the end of the\r\ncollection.\r\n\r\nThis count is only useful for debugging and logging purposes, so you can just\r\nparse the log, insert the different lines in the database and then\r\n\"return 0, 0\".\r\n\r\n## Outputs\r\n\r\nThe output modules perform the actions needed in order to return the data in\r\nthe desired format. What does that mean? Basically, you can do whatever you\r\nwant/can with the output.\r\n\r\nThe output module receives data and processes it in any way you want. The\r\nsupplied examples are designed to print in the CLI, but you can write connectors\r\n to SIEMs, send the results via e-mail... mostly anything you want.\r\n\r\nThe basic structure of an output module is the following:\r\n\r\n```python\r\naid = \"unique_id\"\r\nname = \"name of the output\"\r\ndesc = \"Description of the output\"\r\n\r\ndef launch(data):\r\n    \"\"\"Process data\"\"\"\r\n```\r\nIf you launch a command without specifying any output, it will use the default\r\noutput. The default output is define in the global variable “DEFAULT_OUTPUT” in\r\n*gummer.py*.\r\n\r\n## Analyzers\r\n\r\nAnalyzers are the core of Gummer. You must have in mind that all the value of\r\nthis framework is in the anomalies that you define within the analyzers.\r\n\r\nGummer queries databases to search for the data needed to define the different\r\nanomalies. The data retuned by these anomalies is normally going to be the\r\nresult of a query, but the functionality of the analyzers is only limited by\r\nyour knowledge of python.\r\n\r\nExecuting an analyzer:\r\n\r\n```\r\npython gummer.py –a <analyzer_id> -db <db_connector_id> -o <output_id>\r\n```\r\n\r\n\r\nThe structure of an analyzer is the following:\r\n\r\n```python\r\nimport db_loader\r\n\r\naid = \"unique_id\"\r\nname = \"name of the analyzer\"\r\ndesc = \"Description of the analyzer\"\r\n\r\ndef launch(connector):\r\n    \"\"\"Definition of the analyzer\"\"\"\r\n    query = \"select * from db\"\r\n    data = db_loader.db_query(connector, query)\r\n    return data\r\n```\r\n\r\nWe need to import *db_loader* to be able to connect to the database from which\r\nwe are going to get the data. That is also the reason why the function *launch*\r\nreceives \"connector\". \"connector\" is the module that connects to the database\r\nfrom which the analyzer is going to retrieve the information.\r\n\r\nThe result of the query is stored in the variable data and it is returned to\r\n*gummer.py* in order to be passed to the selected output module. Therefore,\r\nit’s very important that the data type stored in the variable returned by the\r\nanalyzers is compatible with the selected output module.\r\n\r\n## DB Connectors\r\n\r\nThe DB connectors are the modules that allow the other modules to connect to the\r\ndifferent databases.\r\n\r\nThese modules have to be very flexible and execute any query they receive. Thus,\r\ntheir function is basically: to open a connection with the DB, execute a query,\r\nclose the connection, and return the information retrieved. You can find some\r\nexamples of the code in the folder *db_connectors*.\r\n\r\n# Publishing Anomalies/Analyzers.\r\n\r\nThere are only three anomalies published with the framework under the path: \r\nanalyzers/APT_book. These anomalies were published by the Dr. Eric Cole in the\r\nbook [Advanced Persistence Threat](http://www.bookdepository.com/Advanced-Persistent-Threat-Eric-Cole/9781597499491).\r\n\r\nThere are two main reasons why I won't publicly share more anomalies. The first\r\none is because if all these \"rules\" for hunting are made public, malware authors\r\ncan learn how to avoid them, which makes detection even harder. The second one\r\nhas to do with the NDAs that I have signed with the companies I’ve worked for.\r\n\r\nI am definitely not against sharing this kind of knowledge. I just don't like\r\nthe idea of publishing it for everyone to see. There are communities, trusted\r\ncircles, where people exchange Yara rules, IoCs, etc. among the trusted people\r\nwho are part of those groups. If you want to create a community to share\r\nanalyzers, I'll be very happy to join you and support it, but I really just\r\nrefuse to share anomalies in the public repo.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}